{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5C95Js_T9sQ-"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key= 'AIzaSyBc1q004dqycWBSQPW0G1SPZfLzgDY9Oik')  # Replace with your actual Gemini API key\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "import random\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Fix for the LookupError\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Gemini configuration (ensure youâ€™ve authenticated and set up your API key)\n",
        "model = genai.GenerativeModel(\"gemini-pro\")\n",
        "\n",
        "# Extract important nouns from the text\n",
        "def get_noun_keywords(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith(\"NN\") and len(word) > 3]\n",
        "    return list(set(nouns))\n",
        "\n",
        "# Get distractor words from WordNet\n",
        "def get_distractors_wordnet(word):\n",
        "    distractors = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            lemma_name = lemma.name().replace(\"_\", \" \")\n",
        "            if lemma_name.lower() != word.lower():\n",
        "                distractors.add(lemma_name)\n",
        "        if len(distractors) >= 5:\n",
        "            break\n",
        "    return list(distractors)\n",
        "\n",
        "# Fallback to Gemini for distractors\n",
        "def get_distractors_gemini(word):\n",
        "    prompt = f\"Generate 3 plausible but incorrect multiple-choice options (distractors) for the word: '{word}'. Don't include the word itself. Give only options as a list.\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        distractors = response.text.strip().split(\"\\n\")\n",
        "        distractors = [d.strip(\"-â€¢*1234567890. \").strip() for d in distractors if d.strip()]\n",
        "        return distractors[:3]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Combined distractor function\n",
        "def get_distractors(word):\n",
        "    distractors = get_distractors_wordnet(word)\n",
        "    if len(distractors) < 3:\n",
        "        distractors += get_distractors_gemini(word)\n",
        "    return list(set(distractors))[:3]\n",
        "\n",
        "# Create an MCQ from a sentence and answer\n",
        "def create_mcq(sentence, answer):\n",
        "    question = sentence.replace(answer, \"_______\", 1)\n",
        "    distractors = get_distractors(answer)\n",
        "    options = distractors + [answer]\n",
        "    random.shuffle(options)\n",
        "    return question, options, answer\n",
        "\n",
        "# Main logic\n",
        "def generate_mcqs(paragraph):\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    keywords = get_noun_keywords(paragraph)\n",
        "    used_keywords = set()\n",
        "    questions = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for sentence in sentences:\n",
        "            if keyword in sentence:\n",
        "                question, options, answer = create_mcq(sentence, keyword)\n",
        "                if len(options) >= 2:\n",
        "                    questions.append((question, options, answer))\n",
        "                    used_keywords.add(keyword)\n",
        "                break\n",
        "        if len(questions) == 3:\n",
        "            break\n",
        "    return questions\n",
        "\n",
        "# Get user input and display MCQs\n",
        "paragraph = input(\"ðŸ“˜ Enter a paragraph to generate MCQs:\\n\")\n",
        "\n",
        "if paragraph.strip():\n",
        "    mcqs = generate_mcqs(paragraph)\n",
        "    if mcqs:\n",
        "        for i, (q, opts, ans) in enumerate(mcqs, 1):\n",
        "            print(f\"\\nQ{i}: {q}\")\n",
        "            for idx, opt in enumerate(opts):\n",
        "                print(f\"  {chr(97+idx)}) {opt}\")\n",
        "            print(f\"âœ… Answer: {ans}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Could not generate enough unique questions. Try a longer or more diverse paragraph.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Please enter a valid paragraph.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "k6u4MEDcBT5K",
        "outputId": "3c138374-d272-4e87-8fe3-c65b5119cf43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“˜ Enter a paragraph to generate MCQs:\n",
            "Japan is an island nation located in East Asia, known for its rich culture, advanced technology, and unique history. The country consists of four main islandsâ€”Honshu, Hokkaido, Kyushu, and Shikokuâ€”and thousands of smaller ones. Japan has a population of over 125 million people and Tokyo, its capital, is one of the largest metropolitan areas in the world. Historically, Japan was ruled by samurai warriors and governed under the shogunate system for centuries, which lasted until the mid-19th century. The Meiji Restoration in 1868 marked a period of rapid modernization and industrialization, transforming Japan into a major global power. After devastating losses in World War II, Japan adopted a pacifist constitution and rebuilt its economy to become a leading force in electronics, automobiles, and robotics. Japan also maintains deep cultural traditions such as tea ceremonies, sumo wrestling, and festivals that blend Shinto and Buddhist influences, making it a fascinating blend of the old and new\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2006.83ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q1: The country consists of four main islandsâ€”Honshu, _______, Kyushu, and Shikokuâ€”and thousands of smaller ones.\n",
            "  a) Yezo\n",
            "  b) Hokkaido\n",
            "  c) Ezo\n",
            "âœ… Answer: Hokkaido\n",
            "\n",
            "Q2: After devastating losses in World War II, Japan adopted a pacifist constitution and rebuilt its economy to become a leading _______ in electronics, automobiles, and robotics.\n",
            "  a) military force\n",
            "  b) personnel\n",
            "  c) force\n",
            "  d) military group\n",
            "âœ… Answer: force\n",
            "\n",
            "Q3: Japan also maintains deep cultural traditions such as tea ceremonies, sumo wrestling, and festivals that blend Shinto and Buddhist _______, making it a fascinating blend of the old and new\n",
            "  a) mold\n",
            "  b) influence\n",
            "  c) work\n",
            "  d) influences\n",
            "âœ… Answer: influences\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1623.04ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# âœ… Configure your API key\n",
        "genai.configure(api_key=\"AIzaSyBixFBIpOChBixFy1gYZAfFHrxOFG5nosM\")  # Replace with your real key\n",
        "\n",
        "# âœ… Use Gemini 1.5 Flash\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "# âœ… Input paragraph\n",
        "paragraph = input(\"ðŸ“˜ Enter a paragraph to generate MCQs:\\n\")\n",
        "\n",
        "# âœ… Prompt\n",
        "prompt = f\"\"\"\n",
        "Generate 3 multiple-choice questions based on the following paragraph.\n",
        "Each question should have:\n",
        "- A clear question\n",
        "- 4 options (a, b, c, d)\n",
        "- The correct answer highlighted at the end, and print the answer\n",
        "\n",
        "Paragraph:\n",
        "\\\"\\\"\\\"\n",
        "{paragraph}\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# âœ… Generate MCQs\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# âœ… Display result\n",
        "print(\"\\nðŸ“˜ Generated MCQs:\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "oSrMFhp2GZHz",
        "outputId": "6ba3acba-4d9e-46e9-b5cb-0745e20b0aca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Enter a paragraph to generate MCQs:\n",
            "Artificial Intelligence (AI) has become one of the most transformative technologies of the 21st century. It enables machines to perform tasks that typically require human intelligence, such as recognizing speech, translating languages, and playing complex games like chess or Go. A major breakthrough in AI came with the development of deep learning, where artificial neural networks mimic the structure of the human brain to learn from large datasets. Today, AI is embedded in many everyday tools, including search engines, facial recognition systems, chatbots, and smart home devices. Tech companies are investing heavily in AI research, aiming to build more advanced models capable of creativity, empathy, and reasoning. However, there is growing debate around AI safety, regulation, and the future of work as automation accelerates. Experts argue that collaboration between governments, industry, and academia is essential to guide AI development in a way that benefits society as a whole.\n",
            "\n",
            "ðŸ“˜ Generated MCQs:\n",
            "**Question 1:**\n",
            "\n",
            "What is a major breakthrough in AI mentioned in the paragraph?\n",
            "\n",
            "a) The development of faster computer processors\n",
            "b) The creation of the first chatbot\n",
            "c) The development of deep learning\n",
            "d) The invention of the internet\n",
            "\n",
            "**Correct Answer: c)**\n",
            "\n",
            "\n",
            "**Question 2:**\n",
            "\n",
            "According to the paragraph, which of the following is NOT an example of an everyday tool that currently uses AI?\n",
            "\n",
            "a) Search engines\n",
            "b) Facial recognition systems\n",
            "c) Traditional calculators\n",
            "d) Smart home devices\n",
            "\n",
            "**Correct Answer: c)**\n",
            "\n",
            "\n",
            "**Question 3:**\n",
            "\n",
            "What is a key concern highlighted regarding the future of AI, as discussed in the paragraph?\n",
            "\n",
            "a) The cost of AI development\n",
            "b) The lack of skilled AI researchers\n",
            "c) AI safety, regulation, and the future of work\n",
            "d) The limited applications of AI technology\n",
            "\n",
            "**Correct Answer: c)**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIfOwdqVHxpf",
        "outputId": "52eaa645-a70c-4ef0-889c-853ed07b9fc5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure API Key\n",
        "genai.configure(api_key=\"AIzaSyBixFBIpOChBixFy1gYZAfFHrxOFG5nosM\")\n",
        "\n",
        "# Load Gemini 1.5 Flash model\n",
        "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"ðŸ“˜ Gemini MCQ Generator\")\n",
        "st.markdown(\"Enter a paragraph below to generate multiple-choice questions (MCQs).\")\n",
        "\n",
        "paragraph = st.text_area(\"âœï¸ Paragraph Input\", height=200)\n",
        "\n",
        "if st.button(\"Generate MCQs\"):\n",
        "    with st.spinner(\"Generating MCQs...\"):\n",
        "        prompt = f\"\"\"\n",
        "        Generate 3 multiple-choice questions based on the following paragraph.\n",
        "        Each question should have:\n",
        "        - A clear question\n",
        "        - 4 options (a, b, c, d)\n",
        "        - The correct answer highlighted at the end\n",
        "\n",
        "        Paragraph:\n",
        "        \\\"\\\"\\\"{paragraph}\\\"\\\"\\\"\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            st.success(\"âœ… MCQs Generated:\")\n",
        "            st.markdown(response.text)\n",
        "        except Exception as e:\n",
        "            st.error(f\"âŒ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWE0CFhhJ902",
        "outputId": "e1ebd13e-99f9-47f9-b87e-609d2d9571a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2tykSIV146iFJG5JKzBDlVp6osp_4yKYVXP9jkwAZRKkz39QZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZVvZDMoKjLU",
        "outputId": "134bba35-9657-43c4-ec25-18da5db8d9e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Correct way with v3 ngrok config is to specify addr, not port\n",
        "public_url = ngrok.connect(addr=8501)\n",
        "print(\"ðŸŒ Public URL:\", public_url)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo8NtHJ5KCks",
        "outputId": "467bb9df-1414-41d6-90e5-7ebef729e5bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ Public URL: NgrokTunnel: \"https://90a8-34-82-4-24.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ngrok"
      ],
      "metadata": {
        "id": "k72AfLarKPmw"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}