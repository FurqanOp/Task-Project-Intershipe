{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwj8G9hmbzay",
        "outputId": "b4b97f22-110d-481e-d6b5-a41fb2800f7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOQbm7_sZHz-",
        "outputId": "30517f72-771c-42ce-b2ed-44bf14820465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "import random\n",
        "\n",
        "# Ensure required NLTK resources are downloaded\n",
        "def download_nltk_resources():\n",
        "    # Added 'punkt_tab' to the list of resources to download\n",
        "    resources = ['punkt', 'averaged_perceptron_tagger', 'wordnet', 'punkt_tab']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            # Attempt to find the resource first to avoid unnecessary downloads\n",
        "            # Adjusted the find path logic slightly for clarity and accuracy\n",
        "            resource_path = None\n",
        "            if resource == 'punkt' or resource == 'punkt_tab':\n",
        "                resource_path = f'tokenizers/{resource}'\n",
        "            elif resource == 'averaged_perceptron_tagger':\n",
        "                resource_path = f'taggers/{resource}'\n",
        "            elif resource == 'wordnet':\n",
        "                 resource_path = f'corpora/{resource}'\n",
        "\n",
        "            if resource_path:\n",
        "                 nltk.data.find(resource_path)\n",
        "\n",
        "        except LookupError:\n",
        "            print(f\"Downloading {resource}...\")\n",
        "            nltk.download(resource)\n",
        "\n",
        "    # Explicitly download the English version of the tagger if it's still missing\n",
        "    # This part seems less critical for the current error but kept for completeness\n",
        "    try:\n",
        "        nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "    except LookupError:\n",
        "         print(f\"Downloading averaged_perceptron_tagger_eng...\")\n",
        "         nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "download_nltk_resources()\n",
        "\n",
        "def get_noun_keywords(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith(\"NN\") and len(word) > 3]\n",
        "    return list(set(nouns))\n",
        "\n",
        "def get_distractors(word):\n",
        "    distractors = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            lemma_name = lemma.name().replace(\"_\", \" \")\n",
        "            if lemma_name.lower() != word.lower():\n",
        "                distractors.add(lemma_name)\n",
        "        if len(distractors) >= 5:\n",
        "            break\n",
        "    return list(distractors)\n",
        "\n",
        "def create_mcq(sentence, answer):\n",
        "    question = sentence.replace(answer, \"_______\", 1)\n",
        "    distractors = get_distractors(answer)\n",
        "    distractors = random.sample(distractors, min(3, len(distractors)))\n",
        "    options = distractors + [answer]\n",
        "    random.shuffle(options)\n",
        "    return question, options, answer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_text = \"\"\"Japan is an island nation located in East Asia, known for its rich culture, advanced technology, and unique history. The country consists of four main islands—Honshu, Hokkaido, Kyushu, and Shikoku—and thousands of smaller ones. Japan has a population of over 125 million people and Tokyo, its capital, is one of the largest metropolitan areas in the world. Historically, Japan was ruled by samurai warriors and governed under the shogunate system for centuries, which lasted until the mid-19th century. The Meiji Restoration in 1868 marked a period of rapid modernization and industrialization, transforming Japan into a major global power. After devastating losses in World War II, Japan adopted a pacifist constitution and rebuilt its economy to become a leading force in electronics, automobiles, and robotics. Japan also maintains deep cultural traditions such as tea ceremonies, sumo wrestling, and festivals that blend Shinto and Buddhist influences, making it a fascinating blend of the old and new\"\"\"\n",
        "    sentences = sent_tokenize(input_text)\n",
        "    keywords = get_noun_keywords(input_text)\n",
        "    used = set()\n",
        "    questions = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        for word in keywords:\n",
        "            if word in sent and word not in used:\n",
        "                question, options, answer = create_mcq(sent, word)\n",
        "                questions.append((question, options, answer))\n",
        "                used.add(word)\n",
        "                if len(questions) == 3:\n",
        "                    break\n",
        "        if len(questions) == 3:\n",
        "            break\n",
        "\n",
        "    if questions:\n",
        "        for i, (q, opts, ans) in enumerate(questions, 1):\n",
        "            print(f\"Q{i}: {q}\")\n",
        "            for idx, opt in enumerate(opts):\n",
        "                print(f\"  {chr(97+idx)}) {opt}\")\n",
        "            print(f\"Answer: {ans}\\n\")\n",
        "    else:\n",
        "        print(\"Could not generate questions. Try a different paragraph.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1LFbBoF0X1h",
        "outputId": "1178a56e-2330-42d2-b852-f97e29d6b916"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading wordnet...\n",
            "Q1: Japan is an island nation located in _______ Asia, known for its rich culture, advanced technology, and unique history.\n",
            "  a) due east\n",
            "  b) East\n",
            "  c) Orient\n",
            "  d) eastward\n",
            "Answer: East\n",
            "\n",
            "Q2: Japan is an _______ nation located in East Asia, known for its rich culture, advanced technology, and unique history.\n",
            "  a) island\n",
            "Answer: island\n",
            "\n",
            "Q3: Japan is an island nation located in East Asia, known for its rich culture, advanced technology, and unique _______.\n",
            "  a) account\n",
            "  b) history\n",
            "  c) chronicle\n",
            "  d) story\n",
            "Answer: history\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2tykSIV146iFJG5JKzBDlVp6osp_4yKYVXP9jkwAZRKkz39QZ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgrNiYyPhk0R",
        "outputId": "f1a8e24e-be17-4237-b36e-1077075a4725"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ngrok"
      ],
      "metadata": {
        "id": "VTDRCT8Uhclt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "import random\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "def download_nltk_resources():\n",
        "    resources = ['punkt', 'averaged_perceptron_tagger', 'wordnet']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'taggers/{resource}' if resource == 'averaged_perceptron_tagger' else f'corpora/{resource}')\n",
        "        except LookupError:\n",
        "            nltk.download(resource)\n",
        "\n",
        "download_nltk_resources()\n",
        "\n",
        "# Extract important nouns from the text\n",
        "def get_noun_keywords(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    nouns = [word for word, pos in tagged_words if pos.startswith(\"NN\") and len(word) > 3]\n",
        "    return list(set(nouns))\n",
        "\n",
        "# Get distractor words from WordNet\n",
        "def get_distractors(word):\n",
        "    distractors = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            lemma_name = lemma.name().replace(\"_\", \" \")\n",
        "            if lemma_name.lower() != word.lower():\n",
        "                distractors.add(lemma_name)\n",
        "        if len(distractors) >= 5:\n",
        "            break\n",
        "    return list(distractors)\n",
        "\n",
        "# Create an MCQ from a sentence and answer\n",
        "def create_mcq(sentence, answer):\n",
        "    question = sentence.replace(answer, \"_______\", 1)\n",
        "    distractors = get_distractors(answer)\n",
        "    distractors = random.sample(distractors, min(3, len(distractors)))\n",
        "    options = distractors + [answer]\n",
        "    random.shuffle(options)\n",
        "    return question, options, answer\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"🧠 MCQ Generator using NLTK and WordNet\")\n",
        "\n",
        "text_input = st.text_area(\"Enter a paragraph:\", height=200)\n",
        "\n",
        "if st.button(\"Generate MCQs\"):\n",
        "    if text_input:\n",
        "        sentences = sent_tokenize(text_input)\n",
        "        keywords = get_noun_keywords(text_input)\n",
        "        used_keywords = set()\n",
        "        questions = []\n",
        "\n",
        "        for keyword in keywords:\n",
        "            for sentence in sentences:\n",
        "                if keyword in sentence:\n",
        "                    question, options, answer = create_mcq(sentence, keyword)\n",
        "                    if len(options) >= 2:  # ensure we have valid options\n",
        "                        questions.append((question, options, answer))\n",
        "                        used_keywords.add(keyword)\n",
        "                    break  # go to next keyword after 1 question\n",
        "            if len(questions) == 3:\n",
        "                break\n",
        "\n",
        "        if questions:\n",
        "            for i, (q, opts, ans) in enumerate(questions, 1):\n",
        "                st.markdown(f\"**Q{i}: {q}**\")\n",
        "                for idx, opt in enumerate(opts):\n",
        "                    st.write(f\"{chr(97+idx)}) {opt}\")\n",
        "                st.success(f\"✅ Answer: {ans}\")\n",
        "        else:\n",
        "            st.warning(\"Could not generate enough unique questions. Try a longer or more diverse paragraph.\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a paragraph first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOyzqOZ95MAf",
        "outputId": "e7317dac-ea21-4864-8758-8ca730258523"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Launch the app\n",
        "!streamlit run app.py &> /dev/null &\n",
        "\n",
        "# Get public URL\n",
        "# Use 'addr' instead of 'port' to specify the address to connect to\n",
        "public_url = ngrok.connect(addr=8501)\n",
        "print(f\"👉 Click here to open Streamlit app: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU49U4Pb5lrd",
        "outputId": "9edd0c0d-0531-4cd7-8f00-5f8fb05e8dea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👉 Click here to open Streamlit app: NgrokTunnel: \"https://e558-35-196-176-220.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYX12Kw25zON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}